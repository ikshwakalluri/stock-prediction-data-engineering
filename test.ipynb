{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether the symbols exist in the API\n",
    "import os\n",
    "import requests\n",
    "api_key = os.getenv(\"ALPHAVANTAGE_API_KEY\")\n",
    "df=pd.read_csv(\"company_tick_symbols.csv\")\n",
    "for symbol in df['Symbol']:\n",
    "    url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&outputsize=full&apikey={api_key}\"\n",
    "    r = requests.get(url)\n",
    "    data = r.json()\n",
    "    if 'Error Message' in data.keys():\n",
    "        df['Data_Exising']= False\n",
    "        break\n",
    "    else:    \n",
    "        df['Data_Exising']= True\n",
    "        break\n",
    "df.to_csv('company_tick_symbols.csv',index=False)\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data when using Alpha Vantage API and convert it into a DataFrame\n",
    "def pre_process_data(company_name):\n",
    "    \n",
    "    try:\n",
    "        url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={company_name}&outputsize=full&apikey={api_key}\"\n",
    "        r = requests.get(url)\n",
    "        data = r.json()\n",
    "        df=pd.DataFrame(data)\n",
    "        df1=pd.DataFrame()\n",
    "        df.reset_index(inplace=True)\n",
    "        compnay_name=df['Meta Data'][1]\n",
    "        df.drop(['Meta Data'],axis=1,inplace=True)\n",
    "        df.drop(index=df.index[0:5],inplace=True)   \n",
    "        df.reset_index(inplace=True,drop=True)\n",
    "        df1['Date']=df['index'].apply(lambda x: x)\n",
    "        df1['Company_Name']=compnay_name\n",
    "        df1['Open']=df['Time Series (Daily)'].apply(lambda x: x['1. open'])\n",
    "        df1['High']=df['Time Series (Daily)'].apply(lambda x: x['2. high']) \n",
    "        df1['Low']=df['Time Series (Daily)'].apply(lambda x: x['3. low'])\n",
    "        df1['Close']=df['Time Series (Daily)'].apply(lambda x: x['4. close'])\n",
    "        df1['Volume']=df['Time Series (Daily)'].apply(lambda x: x['5. volume'])\n",
    "        return df1\n",
    "    except ValueError:\n",
    "        # print(\"Error in fetching data, check the company name along with the API Key\")\n",
    "        return None\n",
    "df2=pre_process_data('IBM')\n",
    "\n",
    "\n",
    "#Add metadata to the company_tick_symbols_processed.csv file: Here the original file is company_symbols.csv\n",
    "\n",
    "df=pd.read_csv(\"company_tick_symbols_processed.csv\")\n",
    "df1=pd.read_csv(\"company_symbols.csv\")\n",
    "\n",
    "\n",
    "for symbol in df['Symbol']:\n",
    "    df.loc[df['Symbol'] == symbol, 'IPO_Year']=df1['IPO Year'][df1['Symbol']==symbol].values[0]\n",
    "    df.loc[df['Symbol'] == symbol, 'Country']=df1['Country'][df1['Symbol']==symbol].values[0]\n",
    "    df.loc[df['Symbol'] == symbol, 'Industry']=df1['Industry'][df1['Symbol']==symbol].values[0]\n",
    "    \n",
    "df.to_csv('company_tick_symbols_processed.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for A\n",
      "Data for A for year 2000 saved at ./stock_data/company=A/year=2000/A_data_2000.parquet\n",
      "Data for A for year 2001 saved at ./stock_data/company=A/year=2001/A_data_2001.parquet\n",
      "Data for A for year 2002 saved at ./stock_data/company=A/year=2002/A_data_2002.parquet\n",
      "Data for A for year 2003 saved at ./stock_data/company=A/year=2003/A_data_2003.parquet\n",
      "Data for A for year 2004 saved at ./stock_data/company=A/year=2004/A_data_2004.parquet\n",
      "Data for A for year 2005 saved at ./stock_data/company=A/year=2005/A_data_2005.parquet\n",
      "Data for A for year 2006 saved at ./stock_data/company=A/year=2006/A_data_2006.parquet\n",
      "Data for A for year 2007 saved at ./stock_data/company=A/year=2007/A_data_2007.parquet\n",
      "Data for A for year 2008 saved at ./stock_data/company=A/year=2008/A_data_2008.parquet\n",
      "Data for A for year 2009 saved at ./stock_data/company=A/year=2009/A_data_2009.parquet\n",
      "Data for A for year 2010 saved at ./stock_data/company=A/year=2010/A_data_2010.parquet\n",
      "Data for A for year 2011 saved at ./stock_data/company=A/year=2011/A_data_2011.parquet\n",
      "Data for A for year 2012 saved at ./stock_data/company=A/year=2012/A_data_2012.parquet\n",
      "Data for A for year 2013 saved at ./stock_data/company=A/year=2013/A_data_2013.parquet\n",
      "Data for A for year 2014 saved at ./stock_data/company=A/year=2014/A_data_2014.parquet\n",
      "Data for A for year 2015 saved at ./stock_data/company=A/year=2015/A_data_2015.parquet\n",
      "Data for A for year 2016 saved at ./stock_data/company=A/year=2016/A_data_2016.parquet\n",
      "Data for A for year 2017 saved at ./stock_data/company=A/year=2017/A_data_2017.parquet\n",
      "Data for A for year 2018 saved at ./stock_data/company=A/year=2018/A_data_2018.parquet\n",
      "Data for A for year 2019 saved at ./stock_data/company=A/year=2019/A_data_2019.parquet\n",
      "Data for A for year 2020 saved at ./stock_data/company=A/year=2020/A_data_2020.parquet\n",
      "Data for A for year 2021 saved at ./stock_data/company=A/year=2021/A_data_2021.parquet\n",
      "Data for A for year 2022 saved at ./stock_data/company=A/year=2022/A_data_2022.parquet\n",
      "Data for A for year 2023 saved at ./stock_data/company=A/year=2023/A_data_2023.parquet\n",
      "Data for A for year 2024 saved at ./stock_data/company=A/year=2024/A_data_2024.parquet\n",
      "Fetching and storing data for AA\n",
      "Data for AA for year 2000 saved at ./stock_data/company=AA/year=2000/AA_data_2000.parquet\n",
      "Data for AA for year 2001 saved at ./stock_data/company=AA/year=2001/AA_data_2001.parquet\n",
      "Data for AA for year 2002 saved at ./stock_data/company=AA/year=2002/AA_data_2002.parquet\n",
      "Data for AA for year 2003 saved at ./stock_data/company=AA/year=2003/AA_data_2003.parquet\n",
      "Data for AA for year 2004 saved at ./stock_data/company=AA/year=2004/AA_data_2004.parquet\n",
      "Data for AA for year 2005 saved at ./stock_data/company=AA/year=2005/AA_data_2005.parquet\n",
      "Data for AA for year 2006 saved at ./stock_data/company=AA/year=2006/AA_data_2006.parquet\n",
      "Data for AA for year 2007 saved at ./stock_data/company=AA/year=2007/AA_data_2007.parquet\n",
      "Data for AA for year 2008 saved at ./stock_data/company=AA/year=2008/AA_data_2008.parquet\n",
      "Data for AA for year 2009 saved at ./stock_data/company=AA/year=2009/AA_data_2009.parquet\n",
      "Data for AA for year 2010 saved at ./stock_data/company=AA/year=2010/AA_data_2010.parquet\n",
      "Data for AA for year 2011 saved at ./stock_data/company=AA/year=2011/AA_data_2011.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for AA for year 2012 saved at ./stock_data/company=AA/year=2012/AA_data_2012.parquet\n",
      "Data for AA for year 2013 saved at ./stock_data/company=AA/year=2013/AA_data_2013.parquet\n",
      "Data for AA for year 2014 saved at ./stock_data/company=AA/year=2014/AA_data_2014.parquet\n",
      "Data for AA for year 2015 saved at ./stock_data/company=AA/year=2015/AA_data_2015.parquet\n",
      "Data for AA for year 2016 saved at ./stock_data/company=AA/year=2016/AA_data_2016.parquet\n",
      "Data for AA for year 2017 saved at ./stock_data/company=AA/year=2017/AA_data_2017.parquet\n",
      "Data for AA for year 2018 saved at ./stock_data/company=AA/year=2018/AA_data_2018.parquet\n",
      "Data for AA for year 2019 saved at ./stock_data/company=AA/year=2019/AA_data_2019.parquet\n",
      "Data for AA for year 2020 saved at ./stock_data/company=AA/year=2020/AA_data_2020.parquet\n",
      "Data for AA for year 2021 saved at ./stock_data/company=AA/year=2021/AA_data_2021.parquet\n",
      "Data for AA for year 2022 saved at ./stock_data/company=AA/year=2022/AA_data_2022.parquet\n",
      "Data for AA for year 2023 saved at ./stock_data/company=AA/year=2023/AA_data_2023.parquet\n",
      "Data for AA for year 2024 saved at ./stock_data/company=AA/year=2024/AA_data_2024.parquet\n",
      "Fetching and storing data for AAME\n",
      "Data for AAME for year 2000 saved at ./stock_data/company=AAME/year=2000/AAME_data_2000.parquet\n",
      "Data for AAME for year 2001 saved at ./stock_data/company=AAME/year=2001/AAME_data_2001.parquet\n",
      "Data for AAME for year 2002 saved at ./stock_data/company=AAME/year=2002/AAME_data_2002.parquet\n",
      "Data for AAME for year 2003 saved at ./stock_data/company=AAME/year=2003/AAME_data_2003.parquet\n",
      "Data for AAME for year 2004 saved at ./stock_data/company=AAME/year=2004/AAME_data_2004.parquet\n",
      "Data for AAME for year 2005 saved at ./stock_data/company=AAME/year=2005/AAME_data_2005.parquet\n",
      "Data for AAME for year 2006 saved at ./stock_data/company=AAME/year=2006/AAME_data_2006.parquet\n",
      "Data for AAME for year 2007 saved at ./stock_data/company=AAME/year=2007/AAME_data_2007.parquet\n",
      "Data for AAME for year 2008 saved at ./stock_data/company=AAME/year=2008/AAME_data_2008.parquet\n",
      "Data for AAME for year 2009 saved at ./stock_data/company=AAME/year=2009/AAME_data_2009.parquet\n",
      "Data for AAME for year 2010 saved at ./stock_data/company=AAME/year=2010/AAME_data_2010.parquet\n",
      "Data for AAME for year 2011 saved at ./stock_data/company=AAME/year=2011/AAME_data_2011.parquet\n",
      "Data for AAME for year 2012 saved at ./stock_data/company=AAME/year=2012/AAME_data_2012.parquet\n",
      "Data for AAME for year 2013 saved at ./stock_data/company=AAME/year=2013/AAME_data_2013.parquet\n",
      "Data for AAME for year 2014 saved at ./stock_data/company=AAME/year=2014/AAME_data_2014.parquet\n",
      "Data for AAME for year 2015 saved at ./stock_data/company=AAME/year=2015/AAME_data_2015.parquet\n",
      "Data for AAME for year 2016 saved at ./stock_data/company=AAME/year=2016/AAME_data_2016.parquet\n",
      "Data for AAME for year 2017 saved at ./stock_data/company=AAME/year=2017/AAME_data_2017.parquet\n",
      "Data for AAME for year 2018 saved at ./stock_data/company=AAME/year=2018/AAME_data_2018.parquet\n",
      "Data for AAME for year 2019 saved at ./stock_data/company=AAME/year=2019/AAME_data_2019.parquet\n",
      "Data for AAME for year 2020 saved at ./stock_data/company=AAME/year=2020/AAME_data_2020.parquet\n",
      "Data for AAME for year 2021 saved at ./stock_data/company=AAME/year=2021/AAME_data_2021.parquet\n",
      "Data for AAME for year 2022 saved at ./stock_data/company=AAME/year=2022/AAME_data_2022.parquet\n",
      "Data for AAME for year 2023 saved at ./stock_data/company=AAME/year=2023/AAME_data_2023.parquet\n",
      "Data for AAME for year 2024 saved at ./stock_data/company=AAME/year=2024/AAME_data_2024.parquet\n",
      "Fetching and storing data for AAON\n",
      "Data for AAON for year 2000 saved at ./stock_data/company=AAON/year=2000/AAON_data_2000.parquet\n",
      "Data for AAON for year 2001 saved at ./stock_data/company=AAON/year=2001/AAON_data_2001.parquet\n",
      "Data for AAON for year 2002 saved at ./stock_data/company=AAON/year=2002/AAON_data_2002.parquet\n",
      "Data for AAON for year 2003 saved at ./stock_data/company=AAON/year=2003/AAON_data_2003.parquet\n",
      "Data for AAON for year 2004 saved at ./stock_data/company=AAON/year=2004/AAON_data_2004.parquet\n",
      "Data for AAON for year 2005 saved at ./stock_data/company=AAON/year=2005/AAON_data_2005.parquet\n",
      "Data for AAON for year 2006 saved at ./stock_data/company=AAON/year=2006/AAON_data_2006.parquet\n",
      "Data for AAON for year 2007 saved at ./stock_data/company=AAON/year=2007/AAON_data_2007.parquet\n",
      "Data for AAON for year 2008 saved at ./stock_data/company=AAON/year=2008/AAON_data_2008.parquet\n",
      "Data for AAON for year 2009 saved at ./stock_data/company=AAON/year=2009/AAON_data_2009.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for AAON for year 2010 saved at ./stock_data/company=AAON/year=2010/AAON_data_2010.parquet\n",
      "Data for AAON for year 2011 saved at ./stock_data/company=AAON/year=2011/AAON_data_2011.parquet\n",
      "Data for AAON for year 2012 saved at ./stock_data/company=AAON/year=2012/AAON_data_2012.parquet\n",
      "Data for AAON for year 2013 saved at ./stock_data/company=AAON/year=2013/AAON_data_2013.parquet\n",
      "Data for AAON for year 2014 saved at ./stock_data/company=AAON/year=2014/AAON_data_2014.parquet\n",
      "Data for AAON for year 2015 saved at ./stock_data/company=AAON/year=2015/AAON_data_2015.parquet\n",
      "Data for AAON for year 2016 saved at ./stock_data/company=AAON/year=2016/AAON_data_2016.parquet\n",
      "Data for AAON for year 2017 saved at ./stock_data/company=AAON/year=2017/AAON_data_2017.parquet\n",
      "Data for AAON for year 2018 saved at ./stock_data/company=AAON/year=2018/AAON_data_2018.parquet\n",
      "Data for AAON for year 2019 saved at ./stock_data/company=AAON/year=2019/AAON_data_2019.parquet\n",
      "Data for AAON for year 2020 saved at ./stock_data/company=AAON/year=2020/AAON_data_2020.parquet\n",
      "Data for AAON for year 2021 saved at ./stock_data/company=AAON/year=2021/AAON_data_2021.parquet\n",
      "Data for AAON for year 2022 saved at ./stock_data/company=AAON/year=2022/AAON_data_2022.parquet\n",
      "Data for AAON for year 2023 saved at ./stock_data/company=AAON/year=2023/AAON_data_2023.parquet\n",
      "Data for AAON for year 2024 saved at ./stock_data/company=AAON/year=2024/AAON_data_2024.parquet\n",
      "Fetching and storing data for AAPL\n",
      "Data for AAPL for year 2000 saved at ./stock_data/company=AAPL/year=2000/AAPL_data_2000.parquet\n",
      "Data for AAPL for year 2001 saved at ./stock_data/company=AAPL/year=2001/AAPL_data_2001.parquet\n",
      "Data for AAPL for year 2002 saved at ./stock_data/company=AAPL/year=2002/AAPL_data_2002.parquet\n",
      "Data for AAPL for year 2003 saved at ./stock_data/company=AAPL/year=2003/AAPL_data_2003.parquet\n",
      "Data for AAPL for year 2004 saved at ./stock_data/company=AAPL/year=2004/AAPL_data_2004.parquet\n",
      "Data for AAPL for year 2005 saved at ./stock_data/company=AAPL/year=2005/AAPL_data_2005.parquet\n",
      "Data for AAPL for year 2006 saved at ./stock_data/company=AAPL/year=2006/AAPL_data_2006.parquet\n",
      "Data for AAPL for year 2007 saved at ./stock_data/company=AAPL/year=2007/AAPL_data_2007.parquet\n",
      "Data for AAPL for year 2008 saved at ./stock_data/company=AAPL/year=2008/AAPL_data_2008.parquet\n",
      "Data for AAPL for year 2009 saved at ./stock_data/company=AAPL/year=2009/AAPL_data_2009.parquet\n",
      "Data for AAPL for year 2010 saved at ./stock_data/company=AAPL/year=2010/AAPL_data_2010.parquet\n",
      "Data for AAPL for year 2011 saved at ./stock_data/company=AAPL/year=2011/AAPL_data_2011.parquet\n",
      "Data for AAPL for year 2012 saved at ./stock_data/company=AAPL/year=2012/AAPL_data_2012.parquet\n",
      "Data for AAPL for year 2013 saved at ./stock_data/company=AAPL/year=2013/AAPL_data_2013.parquet\n",
      "Data for AAPL for year 2014 saved at ./stock_data/company=AAPL/year=2014/AAPL_data_2014.parquet\n",
      "Data for AAPL for year 2015 saved at ./stock_data/company=AAPL/year=2015/AAPL_data_2015.parquet\n",
      "Data for AAPL for year 2016 saved at ./stock_data/company=AAPL/year=2016/AAPL_data_2016.parquet\n",
      "Data for AAPL for year 2017 saved at ./stock_data/company=AAPL/year=2017/AAPL_data_2017.parquet\n",
      "Data for AAPL for year 2018 saved at ./stock_data/company=AAPL/year=2018/AAPL_data_2018.parquet\n",
      "Data for AAPL for year 2019 saved at ./stock_data/company=AAPL/year=2019/AAPL_data_2019.parquet\n",
      "Data for AAPL for year 2020 saved at ./stock_data/company=AAPL/year=2020/AAPL_data_2020.parquet\n",
      "Data for AAPL for year 2021 saved at ./stock_data/company=AAPL/year=2021/AAPL_data_2021.parquet\n",
      "Data for AAPL for year 2022 saved at ./stock_data/company=AAPL/year=2022/AAPL_data_2022.parquet\n",
      "Data for AAPL for year 2023 saved at ./stock_data/company=AAPL/year=2023/AAPL_data_2023.parquet\n",
      "Data for AAPL for year 2024 saved at ./stock_data/company=AAPL/year=2024/AAPL_data_2024.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load the CSV file with company tickers and metadata\n",
    "df_symbols = pd.read_csv(\"company_tick_symbols_processed.csv\")  # Assume this file has columns like ['Symbol', 'IPO Year', 'Sector', 'Country']\n",
    "\n",
    "# Create a local directory to store the data files\n",
    "output_dir = './stock_data/'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "def save_metadata(company_symbol, metadata):\n",
    "    # Create a metadata file for the company\n",
    "    metadata_dir = os.path.join(output_dir, f'company={company_symbol}/')\n",
    "    os.makedirs(metadata_dir, exist_ok=True)\n",
    "    \n",
    "    # Save metadata as a JSON file in the company folder\n",
    "    metadata_file = os.path.join(metadata_dir, 'metadata.json')\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "    # print(f\"Metadata for {company_symbol} saved at {metadata_file}\")\n",
    "\n",
    "def save_yearly_data(data, company_symbol, year):\n",
    "    # Create partitioned directory for company and year\n",
    "    partition_dir = os.path.join(output_dir, f'company={company_symbol}/year={year}/')\n",
    "    os.makedirs(partition_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the entire year's data to a single Parquet file\n",
    "    file_path = os.path.join(partition_dir, f'{company_symbol}_data_{year}.parquet')\n",
    "    data.to_parquet(file_path, compression='snappy')\n",
    "    print(f\"Data for {company_symbol} for year {year} saved at {file_path}\")\n",
    "\n",
    "def fetch_and_store_data_locally(company_symbol, ipo_year, sector, country,company_name,industry):\n",
    "    # Fetch historical data from 2000-01-01 to 2024-10-01\n",
    "    stock_data = yf.download(company_symbol, start=\"2000-01-01\", end=\"2024-10-01\")\n",
    "    \n",
    "    # Add company metadata and save it\n",
    "    metadata = {\n",
    "        \"Company Name\": company_name,\n",
    "        \"IPO Year\": ipo_year,\n",
    "        \"Sector\": sector,\n",
    "        \"Country\": country,\n",
    "        \"Industry\":industry \n",
    "    }\n",
    "    save_metadata(company_symbol, metadata)\n",
    "\n",
    "    # Group data by year and save yearly data in Parquet format\n",
    "    stock_data['Year'] = stock_data.index.year\n",
    "    for year, data in stock_data.groupby('Year'):\n",
    "        save_yearly_data(data, company_symbol, year)\n",
    "\n",
    "# Loop through the companies and fetch data\n",
    "for index, row in df_symbols.iloc[:5].iterrows():\n",
    "    company_symbol = row['Symbol']\n",
    "    ipo_year = row['IPO_Year']\n",
    "    sector = row['Sector']\n",
    "    country = row['Country']\n",
    "    company_name=row['Company_Name']\n",
    "    industry=row['Industry']\n",
    "\n",
    "    \n",
    "    print(f\"Fetching and storing data for {company_symbol}\")\n",
    "    fetch_and_store_data_locally(company_symbol, ipo_year, sector, country,company_name,industry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"company_tick_symbols_processed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Year'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ikshw\\anaconda3\\envs\\ai\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Year'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mYear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ikshw\\anaconda3\\envs\\ai\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\ikshw\\anaconda3\\envs\\ai\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Year'"
     ]
    }
   ],
   "source": [
    "df['Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import e\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_and_store_data_locally(company_symbol,batch_num):\n",
    "    # Fetch historical data from 2000-01-01 to 2024-10-01\n",
    "    try:\n",
    "        stock_data = yf.download(company_symbol, start=\"2000-01-01\", end=\"2024-10-01\",timeout=30,threads=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {company_symbol}: {e}\")\n",
    "    stock_data.reset_index(inplace=True)\n",
    "    stock_data[\"Year\"] = stock_data[\"Date\"].dt.year\n",
    "    stock_data[\"Month\"] = stock_data[\"Date\"].dt.month\n",
    "    \n",
    "\n",
    "    # Group data by year and save yearly data in Parquet format\n",
    "\n",
    "    for (year, month), data in stock_data.groupby([\"Year\", \"Month\"]):\n",
    "        # Create partitioned directory for company and year\n",
    "        partition_dir = os.path.join(output_dir, f'batch_{batch_num}/company={company_symbol}/year={year}/month={month}/')\n",
    "        os.makedirs(partition_dir, exist_ok=True)\n",
    "\n",
    "        # Save the entire year's data to a single Parquet file\n",
    "        file_path = os.path.join(partition_dir, f'{company_symbol}_data_{year}.parquet')\n",
    "        data.to_parquet(file_path, compression='snappy')\n",
    "\n",
    "\n",
    "def update_stock_data_s3(temp_dir, batch_num):\n",
    "    \n",
    "\n",
    "\n",
    "def process_batches():\n",
    "    df_symbols = pd.read_csv(\"company_tick_symbols_processed.csv\")\n",
    "    df_symbols=df_symbols.head(150)\n",
    "    temp_dir = './stock_data/'\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir)\n",
    "    batch_size=100\n",
    "    start = time.time()\n",
    "    for i in range(0, len(df_symbols), batch_size):\n",
    "        batch_num = i // batch_size+1\n",
    "        batch = df_symbols.iloc[i:i+batch_size]\n",
    "        for index, row in batch.iterrows():\n",
    "            print(f\"Fetching and storing data for {row['Symbol']}\")\n",
    "            company_symbol = row['Symbol']\n",
    "            fetch_and_store_data_locally(company_symbol,batch_num)\n",
    "\n",
    "            # Clean up temp directory after processing\n",
    "        update_stock_data_s3(temp_dir, batch_num)\n",
    "        try:\n",
    "            shutil.rmtree(f\"{temp_dir}/batch_{batch_num}\")  # Remove the entire batch directory\n",
    "            print(f\"Cleaned up temp files for batch {batch_num}\")\n",
    "        except Exception as e:\n",
    "                print(f\"Error during cleanup: {e}\")\n",
    "    print('The program takes ', time.time()-start, 'seconds.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AAME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AAON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ABCB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ABEO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ABEV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ABM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ABT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ACGL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ACHC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ACHV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ACIW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ACNB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ACNT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ACU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ADBE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ADC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ADI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ADM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ADP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ADSK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ADTN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ADX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AEE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AEF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AEG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AEHR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AEIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AEM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AEMD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AEO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AEP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AFG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AFL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AGCO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AGM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AGX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AGYS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AIFF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AIG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AIM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AIN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AIOT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AIR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AIRT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AIV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AJG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AKAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AKR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ALB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ALE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ALG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ALK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ALKS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ALL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ALNT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ALOT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ALTS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ALV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ALX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AMAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AMD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AMED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AMG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AMGN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AMKR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AMRN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AMS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AMSC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AMT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AMWD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AMZN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ANDE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ANF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ANIK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ANIX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ANSS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AORT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AOS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for APA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for APD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for APH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for APOG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for APT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for APTO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ARCB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ARE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ARKR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ARL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ARLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned up temp files for batch 1\n",
      "Fetching and storing data for ARMP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for AROW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ARTNA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ARTW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ARW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ARWR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ASA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ASB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ASG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ASGN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ASH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ASML\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ASRT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ASRV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ASTC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ASTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and storing data for ASUR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprocess_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[53], line 53\u001b[0m, in \u001b[0;36mprocess_batches\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching and storing data for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymbol\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m     company_symbol \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymbol\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 53\u001b[0m     \u001b[43mfetch_and_store_data_locally\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany_symbol\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Clean up temp directory after processing\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[53], line 33\u001b[0m, in \u001b[0;36mfetch_and_store_data_locally\u001b[1;34m(company_symbol, batch_num)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Save the entire year's data to a single Parquet file\u001b[39;00m\n\u001b[0;32m     32\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(partition_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompany_symbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_data_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msnappy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ikshw\\anaconda3\\envs\\ai\\lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ikshw\\anaconda3\\envs\\ai\\lib\\site-packages\\pandas\\core\\frame.py:3113\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   3032\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3033\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[0;32m   3034\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3109\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[0;32m   3110\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[1;32m-> 3113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_parquet(\n\u001b[0;32m   3114\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3115\u001b[0m     path,\n\u001b[0;32m   3116\u001b[0m     engine,\n\u001b[0;32m   3117\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3118\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   3119\u001b[0m     partition_cols\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[0;32m   3120\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3121\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3122\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ikshw\\anaconda3\\envs\\ai\\lib\\site-packages\\pandas\\io\\parquet.py:480\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[0;32m    478\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[1;32m--> 480\u001b[0m impl\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[0;32m    481\u001b[0m     df,\n\u001b[0;32m    482\u001b[0m     path_or_buf,\n\u001b[0;32m    483\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    484\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m    485\u001b[0m     partition_cols\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[0;32m    486\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    487\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    489\u001b[0m )\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[1;32mc:\\Users\\ikshw\\anaconda3\\envs\\ai\\lib\\site-packages\\pandas\\io\\parquet.py:190\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[1;34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m     from_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreserve_index\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m index\n\u001b[1;32m--> 190\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_pandas(df, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfrom_pandas_kwargs)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mattrs:\n\u001b[0;32m    193\u001b[0m     df_metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPANDAS_ATTRS\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(df\u001b[38;5;241m.\u001b[39mattrs)}\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import os\n",
    "\n",
    "def upload_file_to_s3(file_path, s3_key, bucket_name):\n",
    "    # Function to upload a single file to S3\n",
    "    # This is where the file upload logic goes\n",
    "    print(f\"Uploading {file_path} to s3://{bucket_name}/{s3_key}\")\n",
    "\n",
    "def parallel_upload_to_s3(temp_dir, batch_number, bucket_name):\n",
    "    # Create a ThreadPoolExecutor that manages the threads\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        # Walk through all subdirectories and files within the temp_dir\n",
    "        for root, dirs, files in os.walk(temp_dir):\n",
    "            for file in files:\n",
    "                local_file_path = os.path.join(root, file)\n",
    "                s3_key = f\"raw_stock_data/batch_{batch_number}/{file}\"\n",
    "                # Submit the task to be run by one of the threads in the pool\n",
    "                futures.append(executor.submit(upload_file_to_s3, local_file_path, s3_key, bucket_name))\n",
    "\n",
    "        # Wait for all the futures (tasks) to complete\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                # Get the result of the future (in this case, nothing, as upload_file_to_s3 has no return value)\n",
    "                future.result()\n",
    "            except Exception as exc:\n",
    "                print(f\"Generated an exception: {exc}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
